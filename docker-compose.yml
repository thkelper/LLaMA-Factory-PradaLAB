version: '3.8'

services:
  llama-factory:
    build:
      dockerfile: Dockerfile
      context: .
    container_name: llama_factory_latest
    volumes:
      - ./hf_cache:/root/.cache/huggingace/
      - ./data:/app/data
      - ./output:/app/output
    environment:
      - CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    ports:
      - "7860:7860"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: "all"
            capabilities: [gpu]
    restart: unless-stopped
